% ===========================================================================
% ===== Background, introduction

@ONLINE{unesco_online,
  author    = {UNESCO},
  title     = {In Situ Access},
  date      = {2014-10-13},
  url       = {http://www.unesco.org/new/en/culture/themes/underwater-cultural-heritage/access/in-situ-access/},
  urldate   = {2018-11-24},
  langid    = {english}
}

@ARTICLE{unesco,
  journal   = {UNESCO},
  title     = {In Situ Access},
  date      = {2014-10-13},
  url       = {http://www.unesco.org/new/en/culture/themes/underwater-cultural-heritage/access/in-situ-access/},
  urldate   = {2018-11-24},
  langid    = {english}
}

@ARTICLE{inertial_video,
  author    = {Aron, Michael and Simon, Gilles and Berger, Marie-Odile},
  title     = {Use of inertial sensors to support video tracking},
  journal   = {Computer Animation and Virtual Worlds},
  volume    = {18},
  number    = {1},
  pages     = {1},
  date      = {2007},
  keywords  = {augmented reality, camera tracking, sensor fusion},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.161},
  doi       = {10.1002/cav.161},
  urldate   = {2018-11-25}
}

@ARTICLE{fusion01,
  author    = {He, Changyu and Kazanzides, Peter and Sen, Hasan Tutkun and Kim, Sungmin and Liu, Yue},
  title     = {An Inertial and Optical Sensor Fusion Approach for Six Degree-of-Freedom Pose Estimation},
  journal   = {Sensors (Basel, Switzerland)},
  volume    = {15},
  number    = {7},
  date      = {2015-07-08},
  keywords  = {Sensors},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/26184191},
  doi       = {10.3390/s150716448},
  urldate   = {2018-11-25}
}

% ===========================================================================
% ===== Pose estimation

@article{97azuma,
    title={A Survey of Augmented Reality}, 
    volume={6}, 
    DOI={10.1162/pres.1997.6.4.355}, 
    number={4}, 
    journal={Presence: Teleoperators and Virtual Environments}, 
    author={Azuma, Ronald T.}, 
    year={1997}, 
    pages={355–385}
}

@article{17ISMAR,
  title={Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008--2017)},
  author={Kim, Kangsoo and Billinghurst, Mark and Bruder, Gerd and Duh, Henry Been-Lirn and Welch, Gregory F},
  journal={IEEE transactions on visualization and computer graphics},
  volume={24},
  number={11},
  pages={2947--2962},
  year={2018},
  publisher={IEEE},
  doi={10.1109/TVCG.2018.2868591},
  ISSN={1077-2626}
}

@inproceedings{vi-sensor,
  title={A synchronized visual-inertial sensor system with FPGA pre-processing for accurate real-time SLAM},
  author={Nikolic, Janosch and Rehder, Joern and Burri, Michael and Gohl, Pascal and Leutenegger, Stefan and Furgale, Paul T and Siegwart, Roland},
  booktitle={2014 IEEE international conference on robotics and automation (ICRA)},
  pages={431--437},
  year={2014},
  organization={IEEE}
}

@inproceedings{99SoYou,
	address = {Houston, TX, USA},
	title = {Hybrid inertial and vision tracking for augmented reality registration},
	isbn = {978-0-7695-0093-5},
	url = {http://ieeexplore.ieee.org/document/756960/},
	doi = {10.1109/VR.1999.756960},
	language = {en},
	urldate = {2019-04-21},
	booktitle = {Proceedings {IEEE} {Virtual} {Reality} ({Cat}. {No}. 99CB36316)},
	publisher = {IEEE Comput. Soc},
	author = {{Suya You} and Neumann, U. and Azuma, R.},
	year = {1999},
	pages = {260--267},
	file = {Suya You et al. - 1999 - Hybrid inertial and vision tracking for augmented .pdf:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\L2Z8QVW7\\Suya You et al. - 1999 - Hybrid inertial and vision tracking for augmented .pdf:application/pdf}
}

@article{02Pinz,
	title = {Hybrid {Tracking} for {Augmented} {Reality}},
	abstract = {Augmented reality applications demand high quality tracking systems in terms of accuracy, jitter, and lag. This paper describes several approaches for vision-based tracking. Systems which have been implemented range from monocular inside-out to calibrated stereo outside-in tracking. Besides standard CCD camera technology, which yields tracking rates of up to 30 Hz, the use of CMOS cameras has been investigated. A small number of compact targets can be tracked at rates of a few 100 Hz up to several kHz using CMOS cameras with direct access to small windows. Vision-based tracking is very accurate, has low jitter, and is sufficiently fast, but it is too fragile to be used standalone. Thus, several concepts for hybrid tracking have been investigated, combining vision-based tracking with magnetic tracking, or with inertial tracking. The later enables us to implement mobile augmented reality systems based on hybrid inside-out tracking.},
	language = {en},
	author = {Pinz, Axel and Brandner, Markus and Ganster, Harald and Kusej, Albert and Lang, Peter and Ribo, Miguel},
	pages = {8},
	file = {Pinz et al. - Hybrid Tracking for Augmented Reality.pdf:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\957D8X7I\\Pinz et al. - Hybrid Tracking for Augmented Reality.pdf:application/pdf}
}

@article{93Azuma,
    author = {Azuma, Ronald},
    title = {Tracking Requirements for Augmented Reality},
    journal = {Commun. ACM},
    issue_date = {July 1993},
    volume = {36},
    number = {7},
    month = jul,
    year = {1993},
    issn = {0001-0782},
    pages = {50--51},
    numpages = {2},
    url = {http://doi.acm.org/10.1145/159544.159581},
    doi = {10.1145/159544.159581},
    acmid = {159581},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {augmented reality, tracking},
} 

@article{19Cejka,
    title={Detecting Square Markers in Underwater Environments},
    author={{\v{C}}ejka, Jan and Bruno, Fabio and Skarlatos, Dimitrios and Liarokapis, Fotis},
    journal={Remote Sensing},
    volume={11},
    number={4},
    pages={459},
    year={2019},
    publisher={Multidisciplinary Digital Publishing Institute}
}

% Markers
@INPROCEEDINGS{ARToolKit, 
    author={H. {Kato} and M. {Billinghurst}}, 
    booktitle={Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)}, 
    title={Marker tracking and HMD calibration for a video-based augmented reality conferencing system}, 
    year={1999}, 
    pages={85-94}, 
    keywords={augmented reality;calibration;image registration;teleconferencing;groupware;head-up displays;computer displays;video signal processing;marker tracking;HMD calibration;video-based augmented reality conferencing system;overlay;virtual images;real world;remote collaborators;virtual monitors;shared virtual whiteboard;precise virtual image registration;computer vision;fiducial markers;calibration method;optical see-through HMD;3D CSCW;head mounted display;Calibration;Augmented reality;Collaborative work;Virtual reality;Collaboration;Computer interfaces;Computer displays;Humans;Virtual environment;Computer vision}, 
    doi={10.1109/IWAR.1999.803809}, 
    month={Oct}
}

@INPROCEEDINGS{ARTag, 
    author={M. {Fiala}}, 
    booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
    title={ARTag, a fiducial marker system using digital techniques}, 
    year={2005}, 
    volume={2},
    pages={590-596 vol. 2}, 
    keywords={image recognition;augmented reality;image coding;forward error correction;ARTag;fiducial marker system;digital technique;digital camera images;image detection algorithm;augmented reality;bi-tonal planar pattern;checksums;forward error correction;Robustness;Forward error correction;Digital cameras;Detection algorithms;Augmented reality;Robotics and automation;Navigation;Robot vision systems;Codes;Joining processes}, 
    doi={10.1109/CVPR.2005.74}, 
    ISSN={1063-6919}, 
    month={June}
}

@article{ARUco,
    title = "Automatic generation and detection of highly reliable fiducial markers under occlusion",
    journal = "Pattern Recognition",
    volume = "47",
    number = "6",
    pages = "2280 - 2292",
    year = "2014",
    issn = "0031-3203",
    doi = "https://doi.org/10.1016/j.patcog.2014.01.005",
    url = "http://www.sciencedirect.com/science/article/pii/S0031320314000235",
    author = "S. Garrido-Jurado and R. Muñoz-Salinas and F.J. Madrid-Cuevas and M.J. Marín-Jiménez",
    keywords = "Augmented reality, Fiducial marker, Computer vision",
    abstract = "This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem."
}

@INPROCEEDINGS{AprilTag, 
    author={E. {Olson}}, 
    booktitle={2011 IEEE International Conference on Robotics and Automation}, 
    title={AprilTag: A robust and flexible visual fiducial system}, 
    year={2011},
    pages={3400-3407}, 
    keywords={bar codes;computer vision;edge detection;feature extraction;graph theory;image coding;image segmentation;pattern clustering;graph-based image segmentation algorithm;quad extraction method;graph-based clustering method;AprilTag;robust visual fiducial system;flexible visual fiducial system;naturally-occurring features;machine perception;artificial features;controllable experiment;ground truth;2D bar code style tag;feature localization;line detection system;digital coding system;occlusion robustness;warping robustness;lens distortion;ARTag system;Encoding;Visualization;Robustness;Payloads;Detectors;Image segmentation;Robots}, 
    doi={10.1109/ICRA.2011.5979561}, 
    ISSN={1050-4729}, 
    month={May}
}

@INPROCEEDINGS{CircularMarker, 
    author={L. {Naimark} and E. {Foxlin}}, 
    booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, 
    title={Circular data matrix fiducial system and robust image processing for a wearable vision-inertial self-tracker}, 
    year={2002},
    pages={27-36}, 
    keywords={computer vision;optical tracking;augmented reality;sensor fusion;circular data matrix fiducial system;robust image processing;wearable low-power hybrid vision-inertial self-tracker;flexible sensor fusion core architecture;reconfiguration;inertial measurement unit;outward-looking wide-angle smart camera;built-in DSP;2D bar-coded fiducials;black-and-white printer;uninterrupted tracking;real-world lighting conditions;large building;campus;homomorphic image processing algorithms;nonuniform lighting;Robustness;Image processing;Sensor fusion;Smart cameras;Intelligent sensors;Wearable sensors;Prototypes;Measurement units;Digital signal processing;Printers}, 
    doi={10.1109/ISMAR.2002.1115065}, 
    month={Oct}
}

@inproceedings{EllipticMarker,
    title={Robust Detection and Identification of Partially Occluded Circular Markers},
    author={Johannes K{\"o}hler and Alain Pagani and Didier Stricker},
    booktitle={VISAPP},
    year={2010}
}

@ARTICLE{OtherMarker1, 
    author={F. {Bergamasco} and A. {Albarelli} and L. {Cosmo} and E. {Rodolà} and A. {Torsello}}, 
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
    title={An Accurate and Robust Artificial Marker Based on Cyclic Codes}, 
    year={2016}, 
    volume={38}, 
    number={12}, 
    pages={2359-2373}, 
    keywords={calibration;cameras;cyclic codes;pose estimation;artificial marker;general purpose fiducial marker;projective invariance;tag identity;redundant cyclic coded sequence;feature organization;camera calibration;information payload delivery;Feature extraction;Cameras;Robustness;Encoding;Calibration;Pattern recognition;Pose estimation;RUNE tag;fiducial markers;cyclic codes;camera calibration;pose estimation}, 
    doi={10.1109/TPAMI.2016.2519024}, 
    ISSN={0162-8828}, 
    month={Dec}
}

@INPROCEEDINGS{OtherMarker2, 
    author={R. {Bencina} and M. {Kaltenbrunner} and S. {Jorda}}, 
    booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops}, 
    title={Improved Topological Fiducial Tracking in the reacTIVision System}, 
    year={2005},
    pages={99-99}, 
    keywords={Cameras;Instruments;User interfaces;Mirrors;Protocols;Image generation;Evolutionary computation;Real time systems;Streaming media;Prototypes}, 
    doi={10.1109/CVPR.2005.475}, 
    ISSN={2160-7508}, 
    month={Sep.}
}

@INPROCEEDINGS{OtherMarker3, 
    author={M. {Toyoura} and H. {Aruga} and M. {Turk} and X. {Mao}}, 
    booktitle={2013 International Conference on Cyberworlds}, 
    title={Detecting Markers in Blurred and Defocused Images}, 
    year={2013},
    pages={183-190}, 
    keywords={augmented reality;graphics processing units;image restoration;object detection;planar markers detection;blurred image;defocused image;augmented reality system;AR system;marker identification method;high-frequency components;single low-frequency component;mono-spectrum marker;GPU;graphics processing unit;vision applications;Image color analysis;Cameras;Band-pass filters;Brightness;Image segmentation;Image edge detection;Real-time systems;augmented reality;spectrum analysis;planar marker},
    doi={10.1109/CW.2013.58}, 
    month={Oct}
}

@INPROCEEDINGS{OtherMarker4, 
    author={M. {Toyoura} and H. {Aruga} and M. {Turk} and X. {Mao}}, 
    booktitle={2013 International Conference on Cyberworlds}, 
    title={Detecting Markers in Blurred and Defocused Images}, 
    year={2013},
    pages={183-190}, 
    keywords={augmented reality;graphics processing units;image restoration;object detection;planar markers detection;blurred image;defocused image;augmented reality system;AR system;marker identification method;high-frequency components;single low-frequency component;mono-spectrum marker;GPU;graphics processing unit;vision applications;Image color analysis;Cameras;Band-pass filters;Brightness;Image segmentation;Image edge detection;Real-time systems;augmented reality;spectrum analysis;planar marker},
    doi={10.1109/CW.2013.58}, 
    month={Oct}
}

@INPROCEEDINGS{OtherMarker5, 
    author={A. {Xu} and G. {Dudek}}, 
    booktitle={2011 Canadian Conference on Computer and Robot Vision}, 
    title={Fourier Tag: A Smoothly Degradable Fiducial Marker System with Configurable Payload Capacity}, 
    year={2011},
    pages={40-47}, 
    keywords={image coding;image matching;mark scanning equipment;object detection;Fourier tag;fiducial marker system;configurable payload capacity;synthetic image frequency spectrum;marker synthesis;data encoding;partial data extraction;adverse imaging conditions;data degradation;digital encoding;image construction techniques;Payloads;Gray-scale;Frequency domain analysis;Degradation;Pixel;Noise;Data mining;fiducial marker;graceful degradation;Fourier transform;Phase-Shift Keying}, 
    doi={10.1109/CRV.2011.13}, 
    month={May}
}

@INPROCEEDINGS{SIFT, 
    author={D. G. {Lowe}}, 
    booktitle={Proceedings of the Seventh IEEE International Conference on Computer Vision}, 
    title={Object recognition from local scale-invariant features}, 
    year={1999}, 
    volume={2}, 
    pages={1150-1157 vol.2}, 
    keywords={object recognition;feature extraction;computational geometry;image matching;least squares approximations;local scale-invariant features;local image features;3D projection;inferior temporal cortex;primate vision;staged filtering approach;local geometric deformations;blurred image gradients;multiple orientation planes;nearest neighbor indexing method;candidate object matches;low residual least squares solution;unknown model parameters;robust object recognition;cluttered partially occluded images;computation time;Object recognition;Electrical capacitance tomography;Image recognition;Lighting;Neurons;Computer science;Reactive power;Filters;Programmable logic arrays;Layout},
    doi={10.1109/ICCV.1999.790410},
    month={Sep.}
}
    
@InProceedings{SURF,
author="Bay, Herbert
and Tuytelaars, Tinne
and Van Gool, Luc",
editor="Leonardis, Ale{\v{s}}
and Bischof, Horst
and Pinz, Axel",
title="SURF: Speeded Up Robust Features",
booktitle="Computer Vision -- ECCV 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="404--417",
isbn="978-3-540-33833-8"
}

@InProceedings{FAST,
    author="Rosten, Edward
    and Drummond, Tom",
    editor="Leonardis, Ale{\v{s}}
    and Bischof, Horst
    and Pinz, Axel",
    title="Machine Learning for High-Speed Corner Detection",
    booktitle="Computer Vision -- ECCV 2006",
    year="2006",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="430--443",
    isbn="978-3-540-33833-8"
}

@INPROCEEDINGS{17Weidner, 
    author={N. {Weidner} and S. {Rahman} and A. Q. {Li} and I. {Rekleitis}}, 
    booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, 
    title={Underwater cave mapping using stereo vision}, 
    year={2017}, 
    pages={5709-5715}, 
    keywords={cameras;stereo image processing;underwater cave mapping;stereo vision;3D mapping;hydrogeology;water resources;marine archaeology;underwater cave exploration;human divers;robotic technology;underwater vision constraints;natural illumination;harsh contrasts;state estimation packages;stereo camera;cave boundaries;visual odometry algorithm;stereo reconstruction;immersive experience;Cameras;Lighting;Calibration;Visualization;Three-dimensional displays;Sonar navigation;Image edge detection}, 
    doi={10.1109/ICRA.2017.7989672}, 
    month={May},}

@article{18Ferrera,
	title = {Real-time {Monocular} {Visual} {Odometry} for {Turbid} and {Dynamic} {Underwater} {Environments}},
	url = {http://arxiv.org/abs/1806.05842},
	language = {en},
	urldate = {2019-04-28},
	journal = {arXiv:1806.05842 [cs]},
	author = {Ferrera, Maxime and Moras, Julien and Trouvé-Peloux, Pauline and Creuze, Vincent},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.05842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{08ISMAR,
	address = {Cambridge, UK},
	title = {Trends in augmented reality tracking, interaction and display: {A} review of ten years of {ISMAR}},
	isbn = {978-1-4244-2840-3},
	shorttitle = {Trends in augmented reality tracking, interaction and display},
	url = {http://ieeexplore.ieee.org/document/4637362/},
	doi = {10.1109/ISMAR.2008.4637362},
	urldate = {2019-04-21},
	booktitle = {2008 7th {IEEE}/{ACM} {International} {Symposium} on {Mixed} and {Augmented} {Reality}},
	publisher = {IEEE},
	author = {{Feng Zhou} and Duh, Henry Been-Lirn and Billinghurst, Mark},
	month = sep,
	year = {2008},
	pages = {193--202},
}


@article{19Xiao,
	title = {Online {IMU} {Self}-{Calibration} for {Visual}-{Inertial} {Systems}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/7/1624},
	doi = {10.3390/s19071624},
	language = {en},
	number = {7},
	urldate = {2019-04-27},
	journal = {Sensors},
	author = {Xiao, Yao and Ruan, Xiaogang and Chai, Jie and Zhang, Xiaoping and Zhu, Xiaoqing},
	month = apr,
	year = {2019},
	pages = {1624},
}

@book{94Titteron,
	title = {Strapdown inertial navigation technology},
	url = {https://books.google.cz/books?id=WwrCrn54n5cC&lpg=PR15&ots=ds4KQcwuCz&dq=Titterton%2C%20D.%3B%20Weston%2C%20J.%20Strapdown%20Inertial%20Navigation%20Technology%2C%202nd%20ed.%3B%20Institution%20of%20Engineering%20and%20Technology%3A&lr&pg=PR14#v=onepage&q=Titterton,%20D.;%20Weston,%20J.%20Strapdown%20Inertial%20Navigation%20Technology,%202nd%20ed.;%20Institution%20of%20Engineering%20and%20Technology:&f=false},
	author = {Titterton}
}

@misc{novatel,
	title = {{IMU} {Errors} and {Their} {Effects}},
	url = {https://www.novatel.com/assets/Documents/Bulletins/APN064.pdf},
	language = {English},
	urldate = {2019-04-20},
	author = {NovAtel},
	month = feb,
	year = {2014}
}


@inproceedings{14Tedaldi,
	address = {Hong Kong, China},
	title = {A robust and easy to implement method for {IMU} calibration without external equipments},
	isbn = {978-1-4799-3685-4},
	url = {http://ieeexplore.ieee.org/document/6907297/},
	doi = {10.1109/ICRA.2014.6907297},
	language = {en},
	urldate = {2019-04-28},
	booktitle = {2014 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Tedaldi, David and Pretto, Alberto and Menegatti, Emanuele},
	month = may,
	year = {2014},
	pages = {3042--3049}
}


@inproceedings{04Kim,
	address = {Monterey, CA, USA},
	title = {Initial calibration of an inertial measurement unit using an optical position tracking system},
	isbn = {978-0-7803-8416-3},
	url = {http://ieeexplore.ieee.org/document/1308980/},
	doi = {10.1109/PLANS.2004.1308980},
	language = {en},
	urldate = {2019-04-28},
	booktitle = {{PLANS} 2004. {Position} {Location} and {Navigation} {Symposium} ({IEEE} {Cat}. {No}.04CH37556)},
	publisher = {IEEE},
	author = {Kim, A. and Golnaraghi, M.F.},
	year = {2004},
	pages = {96--101},
	file = {Kim and Golnaraghi - 2004 - Initial calibration of an inertial measurement uni.pdf:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\A7DR4UFM\\Kim and Golnaraghi - 2004 - Initial calibration of an inertial measurement uni.pdf:application/pdf}
}


@article{98Lotters,
	title = {98Lotters},
	volume = {68},
	issn = {09244247},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924424798000491},
	doi = {10.1016/S0924-4247(98)00049-1},
	language = {en},
	number = {1-3},
	urldate = {2019-04-27},
	journal = {Sensors and Actuators A: Physical},
	author = {Lötters, J.C. and Schipper, J. and Veltink, P.H. and Olthuis, W. and Bergveld, P.},
	month = jun,
	year = {1998},
	pages = {221--228}
}

@phdthesis{03Nassar,
    author = {Nassar, Sameh},
    year = {2003},
    month = {11},
    title = {Improving the inertial navigation system (INS) error model for INS and INS/DGPS applications [microform] /}
}

@inproceedings{16Rehder,
	address = {Stockholm, Sweden},
	title = {Extending kalibr: {Calibrating} the extrinsics of multiple {IMUs} and of individual axes},
	isbn = {978-1-4673-8026-3},
	shorttitle = {Extending kalibr},
	url = {http://ieeexplore.ieee.org/document/7487628/},
	doi = {10.1109/ICRA.2016.7487628},
	language = {en},
	urldate = {2019-04-28},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Rehder, Joern and Nikolic, Janosch and Schneider, Thomas and Hinzmann, Timo and Siegwart, Roland},
	month = may,
	year = {2016},
	pages = {4304--4311},
	file = {Rehder et al. - 2016 - Extending kalibr Calibrating the extrinsics of mu.pdf:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\5X228GZY\\Rehder et al. - 2016 - Extending kalibr Calibrating the extrinsics of mu.pdf:application/pdf}
}

@book{08Springer,
	title = {Springer {Handbook} of {Robotics}},
	isbn = {978-3-540-23957-4},
	abstract = {With the science of robotics undergoing a major transformation just now, Springer’s new, authoritative handbook on the subject couldn’t have come at a better time. Having broken free from its origins in industry, robotics has been rapidly expanding into the challenging terrain of unstructured environments. Unlike other handbooks that focus on industrial applications, the Springer Handbook of Robotics incorporates these new developments. Just like all Springer Handbooks, it is utterly comprehensive, edited by internationally renowned experts, and replete with contributions from leading researchers from around the world. The handbook is an ideal resource for robotics experts but also for people new to this expanding field.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Siciliano, Bruno and Khatib, Oussama},
	month = may,
	year = {2008},
	note = {Google-Books-ID: Xpgi5gSuBxsC},
	keywords = {Computers / Data Processing, Computers / General, Computers / Intelligence (AI) \& Semantics, Technology \& Engineering / Automation, Technology \& Engineering / Electrical}
}

@inproceedings{99Luinge,
	address = {Atlanta, GA, USA},
	title = {Estimation of orientation with gyroscopes and accelerometers},
	volume = {2},
	isbn = {978-0-7803-5674-0},
	url = {http://ieeexplore.ieee.org/document/803999/},
	doi = {10.1109/IEMBS.1999.803999},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {Proceedings of the {First} {Joint} {BMES}/{EMBS} {Conference}. 1999 {IEEE} {Engineering} in {Medicine} and {Biology} 21st {Annual} {Conference} and the 1999 {Annual} {Fall} {Meeting} of the {Biomedical} {Engineering} {Society} ({Cat}. {No}.99CH37015)},
	publisher = {IEEE},
	author = {Luinge, H.J. and Veltink, P.H. and Baten, C.T.M.},
	year = {1999},
	pages = {844},
}

@article{16Palonen,
	title = {Augmented {Reality} {Based} {Human} {Machine} {Interface} for {Semiautonomous} {Work} {Machines}},
	language = {en},
	author = {Palonen, Tuomo},
	year = {2016},
	pages = {75},
}

@INPROCEEDINGS{13Kerl1, 
    author={C. {Kerl} and J. {Sturm} and D. {Cremers}}, 
    booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
    title={Dense visual SLAM for RGB-D cameras}, 
    year={2013}, 
    pages={2100-2106}, 
    keywords={cameras;entropy;graph theory;image texture;photometry;SLAM (robots);open-source software;g2o framework;graph;loop closure detection;keyframe selection;entropy-based similarity measure;depth error minimization;photometric error minimization;dense visual SLAM method;RGB-D cameras;Cameras;Entropy;Visualization;Simultaneous localization and mapping;Trajectory;Optimization;Covariance matrices}, 
    doi={10.1109/IROS.2013.6696650}, 
    ISSN={2153-0858}, 
    month={Nov}
}

@INPROCEEDINGS{13Kerl2, 
author={C. {Kerl} and J. {Sturm} and D. {Cremers}}, 
booktitle={2013 IEEE International Conference on Robotics and Automation}, 
title={Robust odometry estimation for RGB-D cameras}, 
year={2013}, 
pages={3748-3754}, 
keywords={cameras;filtering theory;image registration;motion estimation;photometry;robust odometry estimation;RGB-D cameras;camera motion estimation;RGB-D images;RGB-D frames;photometric error;nonlinear minimization;coarse-to-fine scheme;image data;robust error function;motion model;temporal filtering;sensors;IMU;computational resources;CPU core;constant memory footprint;open source license;Robustness;Licenses;Software;Sensors;Octrees;Convergence;Jacobian matrices}, 
doi={10.1109/ICRA.2013.6631104}, 
ISSN={1050-4729}, 
month={May}
}

@ARTICLE{15Oskiper, 
    author={T. {Oskiper} and M. {Sizintsev} and V. {Branzoi} and S. {Samarasekera} and R. {Kumar}}, 
    journal={IEEE Transactions on Visualization and Computer Graphics}, 
    title={Augmented Reality Binoculars}, 
    year={2015}, 
    volume={21}, 
    number={5}, 
    pages={611-623}, 
    keywords={augmented reality;computer vision;Global Positioning System;Kalman filters;extended Kalman filter;global positioning system;inertial measurement unit;navigation algorithm;binocular shaped shell;wide field;narrow field;live telescopic imagery;augmented reality binocular system;Cameras;Augmented reality;Calibration;Global Positioning System;Visualization;Training;Robustness;IMU;monocular wide and narrow field of view camera;GPS;inertial navigation;sensor fusion;EKF;IMU;monocular wide and narrow field of view camera;GPS;inertial navigation;sensor fusion;EKF}, 
    doi={10.1109/TVCG.2015.2408612}, 
    ISSN={1077-2626}, 
month={May}
}

@INPROCEEDINGS{15Sweeney, 
    author={C. {Sweeney} and J. {Flynn} and B. {Nuernberger} and M. {Turk} and T. {Höllerer}},
    booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality}, 
    title={Efficient Computation of Absolute Pose for Gravity-Aware Augmented Reality}, 
    year={2015},
    pages={19-24}, 
    keywords={augmented reality;real-time systems;smart phones;gravity-aware augmented reality;multicamera system;absolute pose;vertical direction;vertical vanishing points;computer vision techniques;sensor measurements;smartphone;2D correspondences;3D correspondences;IMU noise;real-time model-based tracking;Google Tango tablet;image localization experiment;Cameras;Three-dimensional displays;Gravity;Augmented reality;Accuracy;Robustness;Google;Absolute pose;multi-camera system;gravity-aware augmented reality;inertial sensor;model-based tracking}, 
    doi={10.1109/ISMAR.2015.20}, 
    month={Sep.}
}

@article{15Leutenegger,
	title = {Keyframe-based visual–inertial odometry using nonlinear optimization},
	volume = {34},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364914554813},
	doi = {10.1177/0278364914554813},
	language = {en},
	number = {3},
	urldate = {2019-04-28},
	journal = {The International Journal of Robotics Research},
	author = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
	month = mar,
	year = {2015},
	pages = {314--334},
}

@article{13Leutenegger,
    author = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
    year = {2014},
    month = {02},
    title = {Keyframe-Based Visual-Inertial Odometry Using Nonlinear Optimization},
    volume = {34},
    journal = {The International Journal of Robotics Research},
    doi = {10.1177/0278364914554813}
}

@article{SLAMII,
	title = {Simultaneous localization and mapping ({SLAM}): part {II}},
	volume = {13},
	issn = {1070-9932},
	shorttitle = {Simultaneous localization and mapping ({SLAM})},
	url = {http://ieeexplore.ieee.org/document/1678144/},
	doi = {10.1109/MRA.2006.1678144},
	language = {en},
	number = {3},
	urldate = {2019-04-27},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Bailey, T. and Durrant-Whyte, H.},
	month = sep,
	year = {2006},
	pages = {108--117}
}

@Article{17Taketomi,
    author="Taketomi, Takafumi
    and Uchiyama, Hideaki
    and Ikeda, Sei",
    title="Visual SLAM algorithms: a survey from 2010 to 2016",
    journal="IPSJ Transactions on Computer Vision and Applications",
    year="2017",
    month="Jun",
    day="02",
    volume="9",
    number="1",
    pages="16",
    abstract="SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.",
    issn="1882-6695",
    doi="10.1186/s41074-017-0027-2",
    url="https://doi.org/10.1186/s41074-017-0027-2"
}

@article{16Younes,
  author    = {Georges Younes and
               Daniel C. Asmar and
               Elie A. Shammas},
  title     = {A survey on non-filter-based monocular Visual {SLAM} systems},
  journal   = {CoRR},
  volume    = {abs/1607.00470},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.00470},
  archivePrefix = {arXiv},
  eprint    = {1607.00470},
  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/YounesAS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{Vuforia,
	title = {Vuforia},
	url = {https://www.ptc.com/en/products/augmented-reality},
	abstract = {Augmented Reality empowers innovators and developers to build immersive AR experiences that transform the way users create, operate and service products.},
	language = {en},
	urldate = {2019-04-30}
}

@misc{ARcore,
	title = {{ARCore}},
	url = {https://developers.google.com/ar/},
	abstract = {With ARCore, build new augmented reality experiences that seamlessly blend the digital and physical worlds. Transform the way people play, shop, learn, create, and experience the world together—at Google scale.},
	language = {en},
	urldate = {2019-04-30},
	journal = {Google Developers},
	file = {Snapshot:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\2DV4WNQY\\ar.html:text/html}
}

@misc{ARkit,
	title = {{ARKit}},
	url = {https://developer.apple.com/arkit/},
	abstract = {Build unparalleled augmented reality experiences for hundreds of millions of users on iOS — the biggest AR platform in the world. With ARKit 2 on iOS 12, your AR apps can now be experienced by multiple users simultaneously, and resumed at a later time in the same state. You can also incorporate real-world objects into your AR experiences, giving your users even greater immersive opportunities.},
	language = {en},
	urldate = {2019-04-30},
	file = {Snapshot:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\SVY3GVAW\\arkit.html:text/html}
}

@article{Comparative,
	title = {Comparative {Study} of {Augmented} {Reality} {Sdk}'s},
	volume = {5},
	issn = {22000011},
	url = {http://www.airccse.org/journal/ijcsa/papers/5115ijcsa02.pdf},
	doi = {10.5121/ijcsa.2015.5102},
	abstract = {Augmented reality (AR) is a technology which provides real time integration of digital content with the information available in real world. Augmented reality enables direct access to implicit information attached with context in real time. Augmented reality enhances our perception of real world by enriching what we see, feel, and hear in the real environment. This paper gives comparative study of various augmented reality software development kits (SDK’s) available to create augmented reality apps. The paper describes how augmented reality is different from virtual reality; working of augmented reality system and different types of tracking used in AR.},
	language = {en},
	number = {1},
	urldate = {2019-04-30},
	journal = {International Journal on Computational Science \& Applications},
	author = {Amin, Dhiraj and Govilkar, Sharvari},
	month = feb,
	year = {2015},
	pages = {11--26},
	file = {Amin and Govilkar - 2015 - Comparative Study of Augmented Reality Sdk's.pdf:C\:\\Users\\Lenovo.LENOVOZ580\\Zotero\\storage\\IHD9FYZQ\\Amin and Govilkar - 2015 - Comparative Study of Augmented Reality Sdk's.pdf:application/pdf}
}

@ARTICLE{14Paull, 
    author={L. {Paull} and S. {Saeedi} and M. {Seto} and H. {Li}}, 
    journal={IEEE Journal of Oceanic Engineering}, 
    title={AUV Navigation and Localization: A Review}, 
    year={2014}, 
    volume={39}, 
    number={1}, 
    pages={131-149}, 
    keywords={autonomous underwater vehicles;Global Positioning System;SLAM (robots);expensive inertial sensors;AUV localization problem;simultaneous localization and mapping technology;underwater communications;radio-frequency signals;GPS;Global Positioning System;autonomous underwater vehicle;Simultaneous localization and mapping;Acoustics;Sonar navigation;Autonomous underwater vehicles (AUVs);marine navigation;simultaneous localization and mapping}, 
    doi={10.1109/JOE.2013.2278891}, 
    ISSN={0364-9059}, 
    month={Jan}
}

@article{18Palomeras,
    title = "AUV homing and docking for remote operations",
    journal = "Ocean Engineering",
    volume = "154",
    pages = "106 - 120",
    year = "2018",
    issn = "0029-8018",
    doi = "https://doi.org/10.1016/j.oceaneng.2018.01.114",
    url = "http://www.sciencedirect.com/science/article/pii/S0029801818301367",
    author = "N. Palomeras and G. Vallicrosa and A. Mallios and J. Bosch and E. Vidal and N. Hurtos and M. Carreras and P. Ridao",
    keywords = "AUV, Docking station, Acoustic localization, Visual tracking, Remote operation"
}

@article{15BoninFont,
    title = "Visual sensing for autonomous underwater exploration and intervention tasks",
    journal = "Ocean Engineering",
    volume = "93",
    pages = "25 - 44",
    year = "2015",
    issn = "0029-8018",
    doi = "https://doi.org/10.1016/j.oceaneng.2014.11.005",
    url = "http://www.sciencedirect.com/science/article/pii/S0029801814004090",
    author = "Francisco Bonin-Font and Gabriel Oliver and Stephan Wirth and Miquel Massot and Pep Lluis Negre and Joan-Pau Beltran",
    keywords = "Underwater intervention and exploration, Vision-based navigation, Visual object detection and tracking"
}